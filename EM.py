#!/usr/bin/python

############################################################
# CSE 5523 starter code (HW#5)
# Yiran Cao
# Based on Starter Code Produced by Prof. Alan Ritter
# Uncomment the According Lines of Code to Generate Figures
############################################################

import random
import math
import sys
import numpy as np
import scipy as sc
import re
import pandas as pd
import matplotlib.pyplot as plt

# GLOBALS/Constants
VAR_INIT = 1

def logExpSum(x):
    # TODO: implement logExpSum
    max_pow = np.max(x)
    x = x - max_pow
    return max_pow + np.log(np.sum(np.exp(x)))

def readTrue(filename='wine-true.data'):
    f = open(filename)
    labels = []
    splitRe = re.compile(r"\s")
    for line in f:
        labels.append(int(splitRe.split(line)[0]))
    return labels

#########################################################################
#Reads and manages data in appropriate format
#########################################################################
class Data:
    def __init__(self, filename):
        self.data = []
        f = open(filename)
        (self.nRows,self.nCols) = [int(x) for x in f.readline().split(" ")]
        for line in f:
            self.data.append([float(x) for x in line.split(" ")])

    #Computers the range of each column (returns a list of min-max tuples)
    def Range(self):
        ranges = []
        for j in range(self.nCols):
            min = self.data[0][j]
            max = self.data[0][j]
            for i in range(1,self.nRows):
                if self.data[i][j] > max:
                    max = self.data[i][j]
                if self.data[i][j] < min:
                    min = self.data[i][j]
            ranges.append((min,max))
        return ranges

    def __getitem__(self,row):
        return self.data[row]

#########################################################################
# Computes EM on a given data set, using the specified number of clusters
# self.parameters is a tuple containing the mean and variance for each gaussian
#########################################################################
class EM:
    def __init__(self, data, nClusters):
        # Initialize parameters randomly...
        self.parameters = []
        self.priors = []        # Cluster priors
        self.nClusters = nClusters
        self.data = data
        self.log_prob = []
        ranges = data.Range()

        for i in range(data.nRows):
            self.log_prob.append(np.ones(3) / 3)
        for i in range(nClusters):
            p = []
            initRow = random.randint(0,data.nRows-1)
            for j in range(data.nCols):
                # Randomly initalize variance in range of data
                p.append((random.uniform(ranges[j][0], ranges[j][1]), VAR_INIT*(ranges[j][1] - ranges[j][0])))
            self.parameters.append(p)

        # Initialize priors uniformly
        for c in range(nClusters):
            self.priors.append(1/float(nClusters))

    def mean_vec(self, cluster):
        miu = [p[0] for p in self.parameters[cluster]]
        return miu

    def cov_matrix(self, cluster):
        std = [p[1] for p in self.parameters[cluster]]
        sigma = []
        ncol = self.data.nCols
        sigma = [[std[i] * std[j] for j in range(ncol)] for i in range(ncol)]
        return sigma


    def LogProb(self, row, cluster, data):
        # TODO: compute probability row was generated by cluster
        miu = [p[0] for p in self.parameters[cluster]]       # mean vector
        delta = [p[1] for p in self.parameters[cluster]]     # standard deviation vector
        temp = 0.5 * np.log(2 * np.pi) + np.log(delta) + (np.array(data.data[row]) - np.array(miu)) ** 2 / (2 * (np.array(delta) ** 2))
        sum = -np.sum(temp)
        return sum

    def LogLikelihood(self, data):
        # TODO: compute log-likelihood of the data
        likelihood = 0
        for i in range(data.nRows):
            power = [self.LogProb(i, cluster, data) + np.log(self.priors[cluster]) for cluster in range(self.nClusters)]
            log_exp_sum = logExpSum(power)
            likelihood = likelihood + log_exp_sum

        return likelihood

    # Compute marginal distributions of hidden variables
    def Estep(self):
        # TODO: E-step
        self.log_prob = []
        nrows = self.data.nRows
        for i in range(nrows):
            cluster_gen_log_prob = [self.LogProb(i, cluster, self.data) for cluster in range(self.nClusters)]
            self.log_prob.append(cluster_gen_log_prob)

        for i in range(nrows):
            powers = self.log_prob[i] + np.log(self.priors)
            reg_term = logExpSum(powers)
            self.log_prob[i] = self.log_prob[i] + np.log(self.priors) - reg_term

    # Update the parameter estimates
    def Mstep(self):
        # TODO: M-step
        nrows = self.data.nRows
        ncols = self.data.nCols
        for cluster in range(self.nClusters):
            # updating mean vectors
            w_vec = np.exp([p[cluster] for p in self.log_prob])
            if(np.sum(w_vec) == 0):
                pass
            else:
                for j in range(ncols):
                    data_col = np.array([d[j] for d in self.data.data])
                    mean_col = np.sum(data_col * w_vec) / np.sum(w_vec)
                    diff_col = (data_col - mean_col) ** 2
                    delta_col = np.sum(diff_col * w_vec) / np.sum(w_vec)
                    std_col = np.sqrt(delta_col)
                    self.parameters[cluster][j] = (mean_col, std_col)
            new_prior = np.average(np.exp(w_vec))
            self.priors[cluster] = new_prior


    def Run(self, maxsteps=100, testData=None, threshold = 0.001):
        # TODO: Implement EM algorithm
        trainLikelihood = self.LogLikelihood(self.data) - 0.01
        iters = []
        llh_train = []
        llh_test = []
        iters.append(0)
        llh_train.append(trainLikelihood)
        llh_test.append(self.LogLikelihood(testData))

        step = 1
        self.Estep()
        trainLikelihood_update = self.LogLikelihood(self.data)
        iters.append(step)
        llh_train.append(trainLikelihood_update)
        llh_test.append(self.LogLikelihood(testData))
        while np.abs(trainLikelihood - trainLikelihood_update) > threshold and step < maxsteps:
            trainLikelihood = trainLikelihood_update
            self.Mstep()
            self.Estep()
            trainLikelihood_update = self.LogLikelihood(self.data)
            llh_train.append(trainLikelihood_update)
            llh_test.append(self.LogLikelihood(testData))
            step = step + 1
            iters.append(step)
        testLikelihood = self.LogLikelihood(testData)

        # Question a: uncomment the following lines to plot likelihood v.s. iterations
        # plt.figure(1, figsize=(9,6))
        # plt.plot(iters, llh_train)
        # plt.xticks(iters)
        # plt.xlabel('# iterations')
        # plt.ylabel('log likelihood, training set')
        # plt.savefig('train.png')
        # plt.figure(2, figsize=(9,6))
        # plt.plot(iters, llh_test)
        # plt.xticks(iters)
        # plt.xlabel('# iterations')
        # plt.ylabel('log likelihood, test set')
        # plt.savefig('test.png')

        return (trainLikelihood, testLikelihood)

if __name__ == "__main__":
    d = Data('wine.train')
    if len(sys.argv) > 1:
        e = EM(d, int(sys.argv[1]))
    else:
        e = EM(d, 3)
    test = Data('wine.test')
    likelihood = e.Run(100, testData=test)

    # Question b: uncomment the following lines to set different seeds and get the plot
    # llh = []
    # for i in range(10):
    #     random.seed(random.randint(0, 100))
    #     e = EM(d, 3)
    #     likelihood = e.Run(100, testData=test)
    #     llh.append(likelihood)
    # plt.figure(3, figsize=(9,6))
    # plt.plot(range(10), [l[0] for l in llh])
    # plt.xticks(range(10))
    # plt.xlabel('seeds')
    # plt.ylabel('converged likelihood, training set')
    # plt.savefig('seeds_train.png')
    # plt.figure(4, figsize=(9, 6))
    # plt.plot(range(10), [l[1] for l in llh])
    # plt.xticks(range(10))
    # plt.xlabel('seeds')
    # plt.ylabel('converged likelihood, test set')
    # plt.savefig('seeds_test.png')
    # print(likelihood)



    # Question c: uncomment the following lines
    # labels = readTrue()
    # test = [np.argmax(p) + 1 for p in e.log_prob]
    # result = pd.DataFrame([labels, test]).transpose().rename(columns={0:'true', 1:'predicted'})
    # result['count'] = 1
    # print result.groupby(['true', 'predicted']).sum()

    #Question d: uncomment the following lines
    # llh = []
    # for i in range(1, 11):
    #     e = EM(d, i)
    #     likelihood = e.Run(100, testData=test)
    #     llh.append(likelihood)
    # plt.figure(5, figsize=(9,6))
    # plt.plot(range(1, 11), [p[0] for p in llh])
    # plt.xticks(range(1,11))
    # plt.xlabel('# clcusters')
    # plt.ylabel('log likelihood, training set')
    # plt.savefig('clusters_train.png')
    # plt.figure(6, figsize=(9, 6))
    # plt.plot(range(1, 11), [p[1] for p in llh])
    # plt.xticks(range(1, 11))
    # plt.xlabel('# clcusters')
    # plt.ylabel('log likelihood, test set')
    # plt.savefig('clusters_test.png')
